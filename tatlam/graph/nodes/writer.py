"""
tatlam/graph/nodes/writer.py - The Writer Node

The Writer generates full scenario candidates using Claude Sonnet 4.5.
It expands seed ideas from the Scout/Curator pipeline into complete,
doctrine-compliant training scenarios.

Key Features:
- Uses Claude Sonnet 4.5 as primary model (Cloud First)
- Local LLM available as fallback
- Incorporates seed ideas from Scout pipeline
- Incorporates Gold examples for quality
- Supports repair mode with Judge critique feedback
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any

from tenacity import (
    before_sleep_log,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from tatlam.graph.state import ScenarioStatus, SwarmState, WorkflowPhase
from tatlam.settings import get_settings

if TYPE_CHECKING:
    from openai import OpenAI

logger = logging.getLogger(__name__)


def _get_clients() -> tuple[OpenAI | None, OpenAI | None, Any]:
    """Get local, cloud (OpenAI), and Anthropic clients."""
    from tatlam.core.llm_factory import (
        ConfigurationError,
        client_cloud,
        client_local,
        create_writer_client,
    )

    local_client = None
    cloud_client = None
    anthropic_client = None

    try:
        local_client = client_local()
    except (ConfigurationError, Exception) as e:
        logger.warning("Local LLM not available: %s", e)

    try:
        cloud_client = client_cloud()
    except (ConfigurationError, Exception) as e:
        logger.debug("Cloud client (OpenAI) not available: %s", e)

    try:
        anthropic_client = create_writer_client()
    except (ConfigurationError, Exception) as e:
        logger.debug("Anthropic client not available: %s", e)

    return local_client, cloud_client, anthropic_client


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    reraise=True,
)
def _call_llm(client: OpenAI, model: str, messages: list[dict], temperature: float = 0.7) -> str:
    """Call LLM with retry logic (OpenAI interface)."""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
    )
    return (response.choices[0].message.content or "").strip()


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=30),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    reraise=True,
)
def _call_anthropic(client: Any, model: str, system: str, user_prompt: str) -> str:
    """Call Anthropic LLM (Claude) with retry logic."""
    response = client.messages.create(
        model=model,
        max_tokens=4096,
        system=system,
        messages=[
            {"role": "user", "content": user_prompt},
        ],
    )
    return (response.content[0].text if response.content else "").strip()


def _get_doctrine_context(venue_type: str = "allenby") -> str:
    """
    Extract key doctrine elements for the Writer prompt.
    This ensures the Writer generates scenarios aligned with the doctrine.
    """
    from tatlam.core.doctrine import TRINITY_DOCTRINE

    doctrine = TRINITY_DOCTRINE

    if venue_type == "jaffa":
        # Surface doctrine
        venue = doctrine.get("venue_surface_jaffa", {})
        safety_rules = """
ðŸš¨ ×›×œ×œ×™ ×‘×¨×–×œ ×œ×™×¤×• (Surface):
- ××™×Ÿ ×œ×’×¢×ª ×‘×—×¤×¥ ×—×©×•×“! (50 ×ž')
- ××•×¤× ×•×¢ 100 ×ž', ×¨×›×‘ 200 ×ž'
- ××™×Ÿ ×©×¢×¨×™× = ×¡×¨×™×§×” ×•×™×–×•××œ×™×ª ×•×ª×’×•×‘×” ×¨×›×•×‘×”
"""
        venue_desc = f"""
ðŸ¢ ×¦×™×¨ ×™×¤×• (×¨×›×‘×ª ×§×œ×” ×¡×‘×™×‘×” ×¤×ª×•×—×”):
- ×¡×•×’: {venue.get('specs', {}).get('type', '×ž×¢×¨×›×ª ×¤×ª×•×—×”')}
- ×›×•×—: ××•×¤× ×•×¢× ×™× + ×ž××‘×˜×—×™ ×¨×›×‘×•×ª (××™×Ÿ ×¢×ž×“×•×ª ×§×‘×•×¢×•×ª)
- ××™×•×: {venue.get('threats_specific', ['×™×¨×™', '××‘× ×™×'])[0]}
"""
    else:
        # Allenby doctrine
        safety = (
            doctrine.get("procedures", {}).get("suspicious_object", {}).get("safety_distances", {})
        )
        venue_data = doctrine.get("venue_allenby", {})

        safety_rules = f"""
ðŸš¨ ×˜×•×•×—×™ ×‘×˜×™×—×•×ª (××œ× ×‘×™):
- ×—×¤×¥ ×—×©×•×“ ×¢×™×¨×•× ×™: {safety.get('object_urban', '50 ×ž×˜×¨')}
- ×¨×›×‘: {safety.get('car', '100 ×ž×˜×¨')} | ×ž×©××™×ª: {safety.get('truck', '400 ×ž×˜×¨')}
"""
        venue_desc = f"""
ðŸ¢ ×ª×—× ×ª ××œ× ×‘×™:
- ×¢×•×ž×§: {venue_data.get('specs', {}).get('depth', '28 ×ž×˜×¨')}
- ×ž×¤×œ×¡ -2: ×©×˜×— ×¡×˜×¨×™×œ×™ (×›×œ ××“× = ××™×¨×•×¢)
"""

    # Legal framework (Shared)
    legal = doctrine.get("legal_framework", {})
    fire = legal.get("open_fire_regulations", {})

    return f"""
ðŸ“š ×ª×•×¨×ª ×”×”×¤×¢×œ×” (DOCTRINE) - ×—×•×‘×”!
{safety_rules}
âš–ï¸ ×¤×ª×™×—×” ×‘××© (Ultima Ratio):
- ×¢×§×¨×•×Ÿ: {fire.get('core_principle', '××ž×¦×¢×™ ××—×¨×•×Ÿ')}
- ×ª× ××™×: ×¡×›× ×ª ×—×™×™× + ××ž×¦×¢×™ + ×›×•×•× ×”
{venue_desc}
"""


def _build_generation_prompt(state: SwarmState, repair_critique: str = "") -> str:
    """
    Build the user prompt for scenario generation.

    Based on build_batch_user_prompt from run_batch.py but adapted
    for the multi-agent context. Integrates doctrine context and scout seeds.
    """
    category = state.category
    count = state.batch_size
    bundle_id = state.bundle_id

    bundle_id = state.bundle_id

    # Determine venue
    venue_type = "allenby"
    if (
        "jaffa" in str(state.category).lower()
        or "surface" in str(state.category).lower()
        or "tachanot-iliyot" in str(state.category).lower()
    ):
        venue_type = "jaffa"

    if "×™×¤×•" in str(state.category) or "×¢×™×œ×™" in str(state.category):
        venue_type = "jaffa"

    # Get doctrine context
    doctrine_context = _get_doctrine_context(venue_type)

    # Repair mode
    repair_section = ""
    if repair_critique:
        repair_section = f"""
âš ï¸ ×ª×™×§×•×Ÿ × ×“×¨×©:
{repair_critique}
"""

    # Scout seeds section (from Scout-Curator pipeline)
    seeds_section = ""
    if state.scout_seeds:
        seeds_list = "\n".join([f"â€¢ {seed}" for seed in state.scout_seeds[:count]])
        seeds_section = f"""
ðŸŒ± ×¨×¢×™×•× ×•×ª ×©× ×‘×—×¨×• ×ž×”×’×©×© (×¤×ª×— ×›×œ ××—×“ ×œ×ª×¨×—×™×© ×ž×œ×):
{seeds_list}

×”× ×—×™×”: ×¤×ª×— ×›×œ ×¨×¢×™×•×Ÿ ×œ×ª×¨×—×™×© ×ž×œ× ×•×ž×¤×•×¨×˜ ×œ×¤×™ ×”×¤×•×¨×ž×˜ ×œ×ž×˜×”.
"""

    # Gold examples
    gold_section = ""
    if state.gold_examples:
        gold_section = f"\n×“×•×’×ž××•×ª Gold:\n{state.gold_examples[:3000]}"

    prompt = f"""{doctrine_context}
{repair_section}
{seeds_section}
batch_id: {bundle_id}
category: {category}
count: {count}

×ž×˜×¨×”: ×¦×•×¨ {count} ×ª×˜×œ×´×ž×™× ×™×™×—×•×“×™×™× ×‘×§×˜×’×•×¨×™×” "{category}".

ðŸ§  ×ž×•×¢×¦×ª ×”×ž×•×ž×—×™× (The Hexagon - ×“×™×•×Ÿ ×¤× ×™×ž×™):
1) ×§×ž×‘"×¥ â€” × ×˜×¨×•×œ ××™×•×, ×—×ª×™×¨×” ×œ×ž×’×¢
2) ×™×•×¢×ž"×© â€” ×—×•×§ ×”×¡×ž×›×•×™×•×ª 2005, ×ž×™×“×ª×™×•×ª
3) ×§×ž"×Ÿ â€” ××™×•× ×”×™×™×—×•×¡ 2025, ××™× ×“×™×§×˜×•×¨×™×
4) ×ž×”× ×“×¡ ×‘×˜×™×—×•×ª â€” ×ž×‘× ×” ××œ× ×‘×™, ×¡×›× ×•×ª ×”×ž×•×Ÿ
5) ×§×¦×™×Ÿ ××ª×™×§×” â€” ×ž× ×™×¢×ª ×¤×¨×•×¤×™×™×œ×™× ×’
6) ×ž× ×”×œ ×”×‘×™×˜×—×•×Ÿ (MAGEN) â€” ×¤×§"×ž, ×ž×¢×’×œ×™ ××‘×˜×—×”

×›×œ×œ×™ ××™×›×•×ª:
- ×©×•× ×•×ª ×ž×œ××” ×‘×™×Ÿ ×ª×˜×œ×´×ž×™×
- ×¢×‘×¨×™×ª ×ž×§×¦×•×¢×™×ª, ×ª×¤×¢×•×œ×™×ª
- ×©×¨×©×¨×ª ×¤×™×§×•×“: ×ž××‘×˜×— â†” ×ž×•×§×“ â†” ×ž×©×˜×¨×”/×—×‘×œ×Ÿ
- × ×ª×•× ×™× ×ž×“×•×™×§×™× ×ž×”×“×•×§×˜×¨×™× ×”

×¤×•×¨×ž×˜ ×œ×›×œ ×ª×˜×œ×´×:
ðŸ§© ×›×•×ª×¨×ª: [×§×¦×¨×”]
ðŸ“‚ ×§×˜×’×•×¨×™×”: {category}
ðŸ”¥ ×¨×ž×ª ×¡×™×›×•×Ÿ: [× ×ž×•×›×”/×‘×™× ×•× ×™×ª/×’×‘×•×”×”/×’×‘×•×”×” ×ž××•×“]
ðŸ“Š ×¨×ž×ª ×¡×‘×™×¨×•×ª: [× ×ž×•×›×”/×‘×™× ×•× ×™×ª/×’×‘×•×”×”]
ðŸ§  ×¨×ž×ª ×ž×•×¨×›×‘×•×ª: [× ×ž×•×›×”/×‘×™× ×•× ×™×ª/×’×‘×•×”×”]

ðŸ“‹ ×¡×™×¤×•×¨ ×ž×§×¨×”:
- ×ž×™×§×•×: [×ž×¤×œ×¡ + ××–×•×¨ ×‘××œ× ×‘×™]
- ×ª×™××•×¨: [×¢×œ×™×œ×”, Actors, ×“×™×œ×ž×”]

ðŸŽ¯ ×©×œ×‘×™ ×ª×’×•×‘×”:
â€¢ ×¦×¢×“ 1-4 ×œ×¤×™ ×”× ×”×œ×™×

ðŸ”« × ×•×”×œ ×¤×ª×™×—×” ×‘××©: [×œ×¤×™ Ultima Ratio]
ðŸ˜· ×©×™×ž×•×© ×‘×ž×¡×›×”: [×›×Ÿ/×œ× + ×¡×™×‘×”]
ðŸŽ¥ ×¨×§×¢ ×ž×‘×¦×¢×™: [××™×¨×•×¢ ××ž×™×ª×™ ××• "××™×Ÿ ×ª×™×¢×•×“"]
ðŸ“Ž ×œ×™× ×§: [×¨×™×§ ×× ××™×Ÿ]
CCTV: [×ž×” ×œ×‘×§×© ×ž×”×ž×•×§×“]
×¡×ž×›×•×™×•×ª: [×—×•×§ ×”×¡×ž×›×•×™×•×ª 2005]
× ×§×•×“×•×ª ×”×›×¨×¢×”: [×“×™×œ×ž×•×ª + ×”×¤× ×™×” ×—×•×§×™×ª]
×ª× ××™ ×”×¡×œ×ž×”: [×ž×ª×™ ×œ×”×¢×œ×•×ª ×¨×ž×”]
×”×¦×œ×—×ª ××™×¨×•×¢: [×§×¨×™×˜×¨×™×•× ×™×]
×›×©×œ ××™×¨×•×¢: [×ž×” ×”×©×ª×‘×©]
×œ×§×—×™×: [2-4 × ×§×•×“×•×ª]
×•×¨×™××¦×™×•×ª: [×’×¨×¡××•×ª]
{gold_section}
"""
    return prompt.strip()


def _load_gold_examples(category: str) -> str:
    """Load gold examples from DB or filesystem."""
    # Import here to avoid circular imports
    from sqlalchemy import func, select

    from tatlam.infra.db import get_session
    from tatlam.infra.models import Scenario

    gold_text = ""
    try:
        with get_session() as session:
            stmt = (
                select(Scenario.title, Scenario.background, Scenario.steps)
                .where(func.lower(Scenario.approved_by) == "gold")
                .order_by(Scenario.id.desc())
                .limit(10)
            )

            rows = session.execute(stmt).fetchall()
            parts = []
            for title, background, steps in rows:
                part = f"### {title}\n×¨×§×¢: {background or ''}\n"
                parts.append(part)
            gold_text = "\n".join(parts)
    except Exception as e:
        logger.debug("Failed to load gold examples: %s", e)

    return gold_text[:4000] if gold_text else ""


def writer_node(state: SwarmState) -> SwarmState:
    """
    Writer Node: Generate scenario candidates.

    This node:
    1. Loads Gold examples for quality guidance
    2. Builds the generation prompt (with repair critique if applicable)
    3. Calls the local LLM (with cloud fallback)
    4. Stores raw drafts in state for the Clerk to validate

    Args:
        state: Current SwarmState

    Returns:
        Updated SwarmState with new draft candidates
    """
    state.log_phase_change(WorkflowPhase.WRITING)
    state.iteration += 1

    logger.info(
        "Writer starting iteration %d for category '%s' (need %d more)",
        state.iteration,
        state.category,
        state.target_count - len(state.approved_scenarios),
    )

    # Load gold examples if not already loaded
    if not state.gold_examples:
        state.gold_examples = _load_gold_examples(state.category)

    # Check for repair mode (scenarios needing rework)
    repair_critique = ""
    scenarios_to_repair = [
        c
        for c in state.candidates
        if c.status == ScenarioStatus.REJECTED and c.attempt_count <= state.max_retries_per_scenario
    ]
    if scenarios_to_repair:
        critiques = [c.critique for c in scenarios_to_repair if c.critique]
        repair_critique = "\n".join(critiques[:3])  # Top 3 critiques

    # Build prompt
    user_prompt = _build_generation_prompt(state, repair_critique)

    # Load system prompt
    from tatlam.core.prompts import get_prompt_manager, memory_addendum

    # Use prompt manager to get dynamic system prompt (injecting venue)
    pm = get_prompt_manager()

    # Infer venue type again for system prompt
    venue_type = "allenby"
    if (
        "jaffa" in str(state.category).lower()
        or "surface" in str(state.category).lower()
        or "tachanot-iliyot" in str(state.category).lower()
    ):
        venue_type = "jaffa"
    if "×™×¤×•" in str(state.category) or "×¢×™×œ×™" in str(state.category):
        venue_type = "jaffa"

    # Build Rule Engine Context
    rule_context = {
        "category": (
            "suspicious_object" if "×—×¤×¥ ×—×©×•×“" in state.category else "general"
        ),  # Map Hebrew to rule keys
        "venue": venue_type,
        "location_type": "surface" if venue_type == "jaffa" else "underground",
        "risk_level": "high",  # default
    }

    # Simple mapping for common Hebrew categories to English rule keys
    if "×—×¤×¥ ×—×©×•×“" in state.category:
        rule_context["category"] = "suspicious_object"
    elif "×¨×›×‘ ×—×©×•×“" in state.category:
        rule_context["category"] = "suspicious_vehicle"

    system_prompt = pm.get_trinity_prompt("writer", venue=venue_type, context=rule_context)
    memory_msg = memory_addendum()

    # Get LLM clients
    local_client, cloud_client, anthropic_client = _get_clients()
    settings = get_settings()

    # Determine primary model based on settings
    use_anthropic = settings.WRITER_MODEL_PROVIDER == "anthropic"
    use_cloud_first = settings.WRITER_MODEL_PROVIDER in ("anthropic", "openai", "google")

    draft_text = ""
    model_used = ""

    # Strategy 1: Anthropic (Claude) - Primary for Writer
    if use_anthropic and anthropic_client:
        try:
            model_used = settings.WRITER_MODEL_NAME
            logger.debug("Calling Cloud Writer (Anthropic): %s", model_used)

            draft_text = _call_anthropic(
                anthropic_client,
                model_used,
                system_prompt,  # Claude accepts system prompt separately
                f"{memory_msg}\n{user_prompt}",  # Append memory to user prompt
            )
        except Exception as e:
            logger.warning("Anthropic Writer failed: %s, falling back...", e)
            state.metrics.llm_errors += 1

    # Strategy 2: Cloud OpenAI/Generic (if Anthropic failed or not selected)
    if not draft_text and use_cloud_first and cloud_client and not use_anthropic:
        try:
            model_used = settings.WRITER_MODEL_NAME or settings.GEN_MODEL
            logger.debug("Calling Cloud Writer (OpenAI): %s", model_used)

            draft_text = _call_llm(
                cloud_client,
                model_used,
                [
                    {"role": "system", "content": system_prompt},
                    memory_msg,
                    {"role": "user", "content": user_prompt},
                ],
            )
        except Exception as e:
            logger.warning("Cloud Writer failed: %s, falling back...", e)
            state.metrics.llm_errors += 1

    # Strategy 3: Local LLM (Primary or Fallback)
    if not draft_text and local_client:
        try:
            model_used = settings.LOCAL_MODEL_NAME
            logger.debug("Calling Local Writer: %s", model_used)
            draft_text = _call_llm(
                local_client,
                model_used,
                [
                    {"role": "system", "content": system_prompt},
                    memory_msg,
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.7,
            )
        except Exception as e:
            logger.warning("Local Writer failed: %s", e)
            state.metrics.llm_errors += 1

    # Strategy 4: Cloud Fallback (if Local was primary and failed)
    if not draft_text and not use_cloud_first and cloud_client:
        try:
            model_used = settings.GEN_MODEL
            logger.debug("Calling Cloud Fallback: %s", model_used)
            draft_text = _call_llm(
                cloud_client,
                model_used,
                [
                    {"role": "system", "content": system_prompt},
                    memory_msg,
                    {"role": "user", "content": user_prompt},
                ],
            )
        except Exception as e:
            logger.error("Cloud Fallback failed: %s", e)
            state.metrics.llm_errors += 1

    if not draft_text:
        state.add_error("Writer failed: all configured LLMs failed")
        return state

    # Store the raw draft for the Clerk to process
    # We store it as a special "raw" candidate
    raw_candidate = state.add_candidate(
        {
            "_raw_text": draft_text,
            "_model": model_used,
            "_is_raw_draft": True,
            "category": state.category,
        }
    )
    raw_candidate.status = ScenarioStatus.DRAFT

    logger.info(
        "Writer completed: generated %d chars of draft text using %s", len(draft_text), model_used
    )

    return state


__all__ = ["writer_node"]
