"""
tatlam/graph/nodes/scout.py - The Scout Node (2-Stage Pipeline)

The Scout uses a 2-stage approach for generating scenario ideas:
1. Stage 1 (Local Qwen): Generate raw, diverse ideas quickly and cheaply
2. Stage 2 (Claude Sonnet): Refine and stabilize ideas, or serve as fallback

Key Features:
- 2-stage pipeline for quality + cost optimization
- Local LLM for creative brainstorming (high temperature)
- Cloud Claude for refinement and structure
- Graceful fallback if local LLM unavailable
"""
from __future__ import annotations

import logging
import re
from typing import TYPE_CHECKING, Any

from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)

from tatlam.graph.state import SwarmState, WorkflowPhase
from tatlam.settings import get_settings
from tatlam.core.doctrine import get_system_prompt

if TYPE_CHECKING:
    from openai import OpenAI
    import anthropic

logger = logging.getLogger(__name__)


def _get_local_client() -> "OpenAI | None":
    """Get the local LLM client."""
    from tatlam.core.llm_factory import client_local, ConfigurationError

    try:
        return client_local()
    except (ConfigurationError, Exception) as e:
        logger.debug("Local LLM not available for Scout: %s", e)
        return None


def _get_anthropic_client() -> "anthropic.Anthropic | None":
    """Get the Anthropic Claude client for Stage 2."""
    from tatlam.core.llm_factory import create_writer_client, ConfigurationError

    try:
        return create_writer_client()
    except (ConfigurationError, Exception) as e:
        logger.debug("Anthropic client not available for Scout Stage 2: %s", e)
        return None


@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    reraise=True,
)
def _call_local_llm(client: "OpenAI", model: str, system: str, prompt: str) -> str:
    """Call local LLM for Stage 1 idea generation."""
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        temperature=0.95,  # Very high creativity for diverse ideas
    )
    return (response.choices[0].message.content or "").strip()


@retry(
    stop=stop_after_attempt(2),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((ConnectionError, TimeoutError)),
    before_sleep=before_sleep_log(logger, logging.WARNING),
    reraise=True,
)
def _call_claude_refinement(client: Any, model: str, raw_ideas: str, category: str) -> str:
    """Call Claude Sonnet for Stage 2 refinement."""
    system_prompt = """××ª×” ××•××—×” ×œ××‘×˜×—×ª ×ª×—×‘×•×¨×” ×¦×™×‘×•×¨×™×ª ×‘××¢×¨×›×ª Trinity.
×ª×¤×§×™×“×š ×œ×§×—×ª ×¨×¢×™×•× ×•×ª ×’×•×œ××™×™× ×œ×ª×¨×—×™×©×™ ××‘×˜×—×” ×•×œ×©×¤×¨ ××•×ª×:

1. ×”×¡×¨ ×¨×¢×™×•× ×•×ª ×œ× ×¨×™××œ×™×¡×˜×™×™× ××• ×œ× ×¨×œ×•×•× ×˜×™×™× ×œ×§×˜×’×•×¨×™×”
2. ×©×¤×¨ × ×™×¡×•×— - ×”×¤×•×š ×›×œ ×¨×¢×™×•×Ÿ ×œ×‘×¨×•×¨ ×•×ª××¦×™×ª×™
3. ×”×•×¡×£ ×¤×¨×˜×™× ××§×¦×•×¢×™×™× (××™×§×•× ×¡×¤×¦×™×¤×™, ××™× ×“×™×§×˜×•×¨×™×)
4. ×•×•×“× ××’×•×•×Ÿ - ×× ×™×© ×—×–×¨×•×ª, ××—×§ ××• ×”×—×œ×£
5. ×©××•×¨ ×¢×œ 15-20 ×”×¨×¢×™×•× ×•×ª ×”×˜×•×‘×™× ×‘×™×•×ª×¨

×¤×œ×˜: ×¨×©×™××ª ×¨×¢×™×•× ×•×ª ××©×•×¤×¨×™×, ×›×œ ××—×“ ×‘×©×•×¨×” × ×¤×¨×“×ª."""

    user_prompt = f"""×§×˜×’×•×¨×™×”: {category}

×¨×¢×™×•× ×•×ª ×’×•×œ××™×™× ×œ×©×™×¤×•×¨:
{raw_ideas}

×”×—×–×¨ ×¨×©×™××ª ×¨×¢×™×•× ×•×ª ××©×•×¤×¨×™× (15-20), ×›×œ ××—×“ ×‘×©×•×¨×” × ×¤×¨×“×ª:"""

    response = client.messages.create(
        model=model,
        max_tokens=2048,
        system=system_prompt,
        messages=[{"role": "user", "content": user_prompt}],
    )
    return (response.content[0].text if response.content else "").strip()


def _build_scout_prompt(category: str, count: int = 25) -> str:
    """Build the prompt for Stage 1 seed generation."""
    return f"""ğŸ¯ ××©×™××”: ×™×™×¦×•×¨ {count} ×¨×¢×™×•× ×•×ª ×’×•×œ××™×™× ×•××’×•×•× ×™× ×œ×ª×¨×—×™×©×™ ××‘×˜×—×”

ğŸ“‚ ×§×˜×’×•×¨×™×”: "{category}"

ğŸ“ ×–×™×¨×ª ×”×¤×¢×•×œ×”: ×ª×—× ×ª ×¨×›×‘×ª ×§×œ×” "××œ× ×‘×™" ×‘×ª×œ ××‘×™×‘
   - ××¤×œ×¡ 0: ×¨×—×•×‘, ×›× ×™×¡×” ×¨××©×™×ª
   - ××¤×œ×¡ -1: ×›×¨×˜×•×¡, ×‘×™×“×•×§, ××‘×•××”
   - ××¤×œ×¡ -2: ×§×•××” ×˜×›× ×™×ª (×¡×˜×¨×™×œ×™)
   - ××¤×œ×¡ -3: ×¨×¦×™×¤×™×

ğŸ“‹ ×›×œ×œ×™×:
1. ×›×œ ×¨×¢×™×•×Ÿ ×‘×©×•×¨×” × ×¤×¨×“×ª
2. ×›×œ ×¨×¢×™×•×Ÿ: ××©×¤×˜ ××—×“ (15-40 ××™×œ×™×)
3. ××’×•×•×Ÿ ××§×¡×™××œ×™ - ×›×œ ×¨×¢×™×•×Ÿ ×©×•× ×” ×œ×—×œ×•×˜×™×Ÿ
4. ×”×ª××§×“ ×‘××™×•××™× ×¨×™××œ×™×¡×˜×™×™× ×•×¨×œ×•×•× ×˜×™×™× ×œ×§×˜×’×•×¨×™×”
5. ×›×œ×•×œ ××™× ×“×™×§×˜×•×¨×™× ×”×ª× ×”×’×•×ª×™×™× ×¡×¤×¦×™×¤×™×™×

ğŸ§  ×˜×™×¤×™× ×œ×™×¦×™×¨×ª×™×•×ª:
- ×—×©×•×‘ ×¢×œ ×©×¢×•×ª ×©×•× ×•×ª ×‘×™×•×
- ×—×©×•×‘ ×¢×œ ××–×’ ××•×•×™×¨ ×•×¢×•× ×•×ª
- ×—×©×•×‘ ×¢×œ ×¡×•×’×™ ×× ×©×™× ×©×•× ×™×
- ×—×©×•×‘ ×¢×œ ×ª×¨×—×™×©×™ ×§×¦×” × ×“×™×¨×™×

×“×•×’×××•×ª ×œ×¤×•×¨××˜:
â€¢ ××“× ×¢× ××¢×™×œ ×’×©× ××¨×•×š ×‘×™×•× ×©××©×™ ×¢×•××“ ×œ×™×“ ×¢××•×“ ×‘××‘×•××” ×•××‘×™×˜ ×¡×‘×™×‘×• ×‘×¢×¦×‘× ×•×ª
â€¢ ×™×œ×“ ×›×‘×Ÿ 10 ××¡×ª×•×‘×‘ ×œ×‘×“×• ×‘×¨×¦×™×£ ×œ×œ× ×œ×™×•×•×™ ××‘×•×’×¨ ×•× ×¨××” ××‘×•×œ×‘×œ
â€¢ ×¨×—×¤×Ÿ ×œ×‘×Ÿ ×§×˜×Ÿ ××¨×—×£ ××¢×œ ×”×›× ×™×¡×” ×‘×’×•×‘×” 15 ××˜×¨ ×•××™× ×• ×–×–

×¦×•×¨ {count} ×¨×¢×™×•× ×•×ª ×—×“×©×™× ×•××’×•×•× ×™×:"""


def _build_fallback_prompt(category: str, count: int = 20) -> str:
    """Build prompt for Claude-only fallback when local LLM unavailable."""
    return f"""××ª×” ××•××—×” ×œ×™×¦×™×¨×ª ×ª×¨×—×™×©×™ ××™××•×Ÿ ×œ××‘×˜×—×ª ×ª×—×‘×•×¨×” ×¦×™×‘×•×¨×™×ª.

×¦×•×¨ {count} ×¨×¢×™×•× ×•×ª ××’×•×•× ×™× ×œ×ª×¨×—×™×©×™ ××‘×˜×—×” ×‘×§×˜×’×•×¨×™×”: "{category}"

×”×–×™×¨×”: ×ª×—× ×ª ×¨×›×‘×ª ×§×œ×” "××œ× ×‘×™" ×‘×ª×œ ××‘×™×‘ (4 ××¤×œ×¡×™×)

×›×œ×œ×™×:
- ×›×œ ×¨×¢×™×•×Ÿ ×‘×©×•×¨×” × ×¤×¨×“×ª
- ×›×œ ×¨×¢×™×•×Ÿ: ××©×¤×˜ ××—×“ ×¢× ××™× ×“×™×§×˜×•×¨×™× ×¡×¤×¦×™×¤×™×™×
- ××’×•×•×Ÿ ××§×¡×™××œ×™

×”×—×–×¨ ×¨×©×™××ª {count} ×¨×¢×™×•× ×•×ª:"""


def _parse_seeds(raw_text: str) -> list[str]:
    """Parse the raw LLM output into a list of seeds."""
    lines = raw_text.strip().split("\n")
    seeds = []
    
    for line in lines:
        # Clean the line
        line = line.strip()
        # Remove common prefixes like "1.", "-", "*", "â€¢"
        line = re.sub(r'^[\d]+[.)]\s*', '', line)
        line = re.sub(r'^[-*â€¢]\s*', '', line)
        line = line.strip()
        
        # Filter valid seeds (reasonable length)
        if 15 < len(line) < 250:
            seeds.append(line)
    
    return seeds


def scout_node(state: SwarmState) -> SwarmState:
    """
    Scout Node: 2-Stage idea generation pipeline.
    
    Stage 1 (Local Qwen): Generate raw, creative ideas quickly
    Stage 2 (Claude Sonnet): Refine items and ensure quality
    
    If Local LLM unavailable, Claude serves as complete fallback.
    
    Args:
        state: Current SwarmState
        
    Returns:
        Updated SwarmState with scout_seeds populated
    """
    state.log_phase_change(WorkflowPhase.SCOUTING)
    
    settings = get_settings()
    seed_count = max(25, state.batch_size * 3)
    
    logger.info(
        "Scout starting 2-stage pipeline for category '%s' (target: %d seeds)",
        state.category, seed_count
    )
    
    # Get clients
    local_client = _get_local_client()
    anthropic_client = _get_anthropic_client()
    
    raw_ideas = ""
    refined_ideas = ""
    
    # === STAGE 1: Local Qwen - Raw Idea Generation ===
    if local_client:
        try:
            logger.info("Scout Stage 1: Generating raw ideas with Local Qwen")
            
            system_prompt = "××ª×” ×™×•×¦×¨ ×¨×¢×™×•× ×•×ª ×™×¦×™×¨×ª×™ ×•××’×•×•×Ÿ. ×ª×¤×§×™×“×š ×œ×”×¦×™×¢ ×¨×¢×™×•× ×•×ª ×××ª×’×¨×™× ×•×œ× ×©×’×¨×ª×™×™× ×œ×ª×¨×—×™×©×™ ××‘×˜×—×”."
            user_prompt = _build_scout_prompt(state.category, seed_count)
            
            raw_ideas = _call_local_llm(
                local_client,
                settings.LOCAL_MODEL_NAME,
                system_prompt,
                user_prompt
            )
            
            logger.info("Scout Stage 1 complete: %d chars of raw ideas", len(raw_ideas))
            
        except Exception as e:
            logger.warning("Scout Stage 1 failed: %s", e)
            state.metrics.llm_errors += 1
            raw_ideas = ""
    
    # === STAGE 2: Claude Sonnet - Refinement ===
    if anthropic_client:
        try:
            if raw_ideas:
                # Refinement mode: improve local ideas
                logger.info("Scout Stage 2: Refining ideas with Claude Sonnet")
                refined_ideas = _call_claude_refinement(
                    anthropic_client,
                    settings.WRITER_MODEL_NAME,  # Claude Sonnet 4.5
                    raw_ideas,
                    state.category
                )
            else:
                # Fallback mode: generate from scratch
                logger.info("Scout Stage 2: Fallback - generating with Claude Sonnet")
                fallback_prompt = _build_fallback_prompt(state.category, 20)
                
                response = anthropic_client.messages.create(
                    model=settings.WRITER_MODEL_NAME,
                    max_tokens=2048,
                    system="××ª×” ××•××—×” ×œ×™×¦×™×¨×ª ×ª×¨×—×™×©×™ ××™××•×Ÿ ×œ××‘×˜×—×ª ×ª×—×‘×•×¨×” ×¦×™×‘×•×¨×™×ª.",
                    messages=[{"role": "user", "content": fallback_prompt}],
                )
                refined_ideas = (response.content[0].text if response.content else "").strip()
            
            logger.info("Scout Stage 2 complete: %d chars refined", len(refined_ideas))
            
        except Exception as e:
            logger.warning("Scout Stage 2 failed: %s", e)
            state.metrics.llm_errors += 1
            # Use raw ideas if refinement failed
            refined_ideas = raw_ideas
    else:
        # No Claude available - use raw ideas directly
        refined_ideas = raw_ideas
    
    # === Parse and store seeds ===
    final_text = refined_ideas or raw_ideas
    
    if final_text:
        seeds = _parse_seeds(final_text)
        state.scout_seeds = seeds
        logger.info(
            "Scout pipeline complete: %d seeds generated (Local: %s, Claude: %s)",
            len(seeds),
            "âœ“" if local_client else "âœ—",
            "âœ“" if anthropic_client else "âœ—"
        )
    else:
        logger.warning("Scout failed: No ideas generated, continuing without seeds")
    
    return state


__all__ = ["scout_node"]
