"""
tatlam/graph/state.py - SwarmState: The Brain of the Multi-Agent System

This module defines the shared state that flows through all nodes in the LangGraph.
The state design follows these principles:

1. Immutability: State updates create new instances
2. Observability: All operations are logged with metrics
3. Type Safety: Full Pydantic validation
4. Compatibility: Maps to existing SCENARIO_BUNDLE_SCHEMA and Scenario ORM
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any
from uuid import uuid4

logger = logging.getLogger(__name__)


class WorkflowPhase(str, Enum):
    """Current phase of the workflow."""

    INIT = "init"
    SCOUTING = "scouting"  # Scout generating ideas
    CURATING = "curating"  # Curator filtering ideas
    WRITING = "writing"
    FORMATTING = "formatting"
    DEDUPLICATING = "deduplicating"
    JUDGING = "judging"
    REPAIRING = "repairing"
    ARCHIVING = "archiving"
    COMPLETE = "complete"
    ERROR = "error"


class ScenarioStatus(str, Enum):
    """Status of an individual scenario candidate."""

    DRAFT = "draft"  # Just generated by Writer
    FORMATTED = "formatted"  # Passed Clerk validation
    UNIQUE = "unique"  # Passed deduplication
    JUDGE_APPROVED = "judge_approved"  # Passed Judge Iron Dome threshold, pending user
    USER_APPROVED = "user_approved"  # User explicitly approved
    APPROVED = "approved"  # Passed Judge threshold (legacy compatibility)
    REJECTED = "rejected"  # Failed Judge threshold
    REPAIRED = "repaired"  # Rewritten after feedback
    ARCHIVED = "archived"  # Saved to database


@dataclass
class ScenarioCandidate:
    """
    A single scenario being processed through the workflow.

    Tracks the scenario's journey from draft to archived, including
    all feedback and score history for observability.
    """

    # Core data
    data: dict[str, Any]

    # Tracking
    id: str = field(default_factory=lambda: str(uuid4())[:8])
    status: ScenarioStatus = ScenarioStatus.DRAFT

    # Quality metrics
    score: float = 0.0
    critique: str = ""

    # History for debugging
    attempt_count: int = 1
    feedback_history: list[str] = field(default_factory=list)
    score_history: list[float] = field(default_factory=list)

    # Timestamps
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    updated_at: str = field(default_factory=lambda: datetime.now().isoformat())

    @property
    def title(self) -> str:
        """Get the scenario title."""
        return self.data.get("title", "")

    @property
    def category(self) -> str:
        """Get the scenario category."""
        return self.data.get("category", "")

    def add_feedback(self, critique: str, score: float) -> None:
        """Record feedback from the Judge."""
        self.critique = critique
        self.score = score
        self.feedback_history.append(critique)
        self.score_history.append(score)
        self.attempt_count += 1
        self.updated_at = datetime.now().isoformat()

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "id": self.id,
            "data": self.data,
            "status": self.status.value,
            "score": self.score,
            "critique": self.critique,
            "attempt_count": self.attempt_count,
            "feedback_history": self.feedback_history,
            "score_history": self.score_history,
            "created_at": self.created_at,
            "updated_at": self.updated_at,
        }


@dataclass
class SwarmMetrics:
    """
    Operational metrics for observability.

    These metrics enable monitoring and debugging of the workflow.
    """

    # Counts
    total_generated: int = 0
    total_approved: int = 0
    total_rejected: int = 0
    total_repaired: int = 0
    total_duplicates_skipped: int = 0

    # Timing
    start_time: str = field(default_factory=lambda: datetime.now().isoformat())
    end_time: str = ""

    # Quality
    average_score: float = 0.0
    highest_score: float = 0.0
    lowest_score: float = 100.0

    # Errors
    llm_errors: int = 0
    parse_errors: int = 0

    def update_score_stats(self, scores: list[float]) -> None:
        """Update score statistics from a list of scores."""
        if not scores:
            return
        self.average_score = sum(scores) / len(scores)
        self.highest_score = max(scores)
        self.lowest_score = min(scores)

    def finalize(self) -> None:
        """Mark the workflow as complete and record end time."""
        self.end_time = datetime.now().isoformat()

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for logging/serialization."""
        return {
            "total_generated": self.total_generated,
            "total_approved": self.total_approved,
            "total_rejected": self.total_rejected,
            "total_repaired": self.total_repaired,
            "total_duplicates_skipped": self.total_duplicates_skipped,
            "average_score": round(self.average_score, 2),
            "highest_score": round(self.highest_score, 2),
            "lowest_score": round(self.lowest_score, 2),
            "llm_errors": self.llm_errors,
            "parse_errors": self.parse_errors,
            "start_time": self.start_time,
            "end_time": self.end_time,
        }


@dataclass
class SwarmState:
    """
    The global state that flows through all LangGraph nodes.

    This state is the "brain" of the workflow - it tracks:
    - Mission scope (category, target count)
    - All scenario candidates and their status
    - Operational metrics for observability
    - Error handling

    Design Notes:
    - Aligns with SCENARIO_BUNDLE_SCHEMA from brain.py
    - Compatible with Scenario ORM from models.py
    - Supports both sync and async execution
    """

    # === Mission Scope ===
    bundle_id: str = field(
        default_factory=lambda: f"BUNDLE-{datetime.now().strftime('%Y%m%d-%H%M')}"
    )
    category: str = ""
    target_count: int = 5

    # === Configuration ===
    score_threshold: float = 70.0  # Minimum score to approve
    max_retries_per_scenario: int = 2  # Max repair attempts
    batch_size: int = 8  # Initial candidates per generation
    diversity_threshold: float = 0.92  # Max similarity for dedup

    # === The Workbox ===
    candidates: list[ScenarioCandidate] = field(default_factory=list)

    # Quick access lists (derived from candidates)
    @property
    def approved_scenarios(self) -> list[ScenarioCandidate]:
        """Scenarios that passed the Judge (includes JUDGE_APPROVED and USER_APPROVED)."""
        return [c for c in self.candidates if c.status in (
            ScenarioStatus.APPROVED,
            ScenarioStatus.JUDGE_APPROVED,
            ScenarioStatus.USER_APPROVED,
        )]

    @property
    def rejected_scenarios(self) -> list[ScenarioCandidate]:
        """Scenarios that failed the Judge."""
        return [c for c in self.candidates if c.status == ScenarioStatus.REJECTED]

    @property
    def pending_scenarios(self) -> list[ScenarioCandidate]:
        """Scenarios still being processed."""
        return [
            c
            for c in self.candidates
            if c.status
            not in (
                ScenarioStatus.APPROVED,
                ScenarioStatus.JUDGE_APPROVED,
                ScenarioStatus.USER_APPROVED,
                ScenarioStatus.REJECTED,
                ScenarioStatus.ARCHIVED,
            )
        ]

    @property
    def needs_more(self) -> bool:
        """Check if we need to generate more scenarios."""
        return len(self.approved_scenarios) < self.target_count

    # === Workflow Control ===
    current_phase: WorkflowPhase = WorkflowPhase.INIT
    iteration: int = 0
    max_iterations: int = 5  # Prevent infinite loops

    # === Operational Metadata ===
    metrics: SwarmMetrics = field(default_factory=SwarmMetrics)
    errors: list[str] = field(default_factory=list)

    # === Gold Examples (for context) ===
    gold_examples: str = ""  # Loaded from DB/files

    # === Scout Seeds (for Scout-Curator pipeline) ===
    scout_seeds: list[str] = field(default_factory=list)  # Raw ideas from Scout

    # === Logging ===
    def log_phase_change(self, new_phase: WorkflowPhase) -> None:
        """Log a phase transition for observability."""
        old_phase = self.current_phase
        self.current_phase = new_phase
        logger.info(
            "Phase transition: %s â†’ %s (iteration=%d, approved=%d/%d)",
            old_phase.value,
            new_phase.value,
            self.iteration,
            len(self.approved_scenarios),
            self.target_count,
        )

    def add_candidate(self, data: dict[str, Any]) -> ScenarioCandidate:
        """Add a new candidate to the workbox."""
        candidate = ScenarioCandidate(data=data)
        self.candidates.append(candidate)
        self.metrics.total_generated += 1
        logger.debug("Added candidate: %s", candidate.title)
        return candidate

    def add_error(self, error: str) -> None:
        """Record an error for debugging."""
        self.errors.append(f"[{datetime.now().isoformat()}] {error}")
        logger.error("SwarmState error: %s", error)

    def to_bundle_dict(self) -> dict[str, Any]:
        """
        Convert to bundle format compatible with existing code.

        This enables seamless integration with insert_bundle() and other
        existing functions that expect the SCENARIO_BUNDLE_SCHEMA format.
        """
        return {
            "bundle_id": self.bundle_id,
            "scenarios": [c.data for c in self.approved_scenarios],
        }

    def get_summary(self) -> dict[str, Any]:
        """Get a summary of the current state for logging."""
        return {
            "bundle_id": self.bundle_id,
            "category": self.category,
            "phase": self.current_phase.value,
            "iteration": self.iteration,
            "candidates_total": len(self.candidates),
            "approved": len(self.approved_scenarios),
            "rejected": len(self.rejected_scenarios),
            "pending": len(self.pending_scenarios),
            "target": self.target_count,
            "needs_more": self.needs_more,
            "metrics": self.metrics.to_dict(),
            "errors_count": len(self.errors),
        }


__all__ = [
    "SwarmState",
    "ScenarioCandidate",
    "ScenarioStatus",
    "SwarmMetrics",
    "WorkflowPhase",
]
