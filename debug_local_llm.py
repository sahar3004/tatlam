#!/usr/bin/env python3
"""
Debug script for testing local Llama server connectivity.

This script verifies:
1. The local Llama server is running and accessible
2. The expected model name matches what the server exposes
3. The model can generate Hebrew text responses

Usage:
    python3 debug_local_llm.py
"""

import sys
from openai import OpenAI

# ×”×’×“×¨×•×ª (×ª×•××ž×•×ª ×œ×ž×” ×©×§×‘×¢× ×•)
BASE_URL = "http://127.0.0.1:8080/v1"
API_KEY = "sk-no-key-required"
EXPECTED_MODEL = "llama-3.3-70b-instruct"

print(f"ðŸ•µï¸  Checking connection to Local Llama at {BASE_URL}...")

try:
    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)

    # 1. ×‘×“×™×§×ª ×¨×©×™×ž×ª ×ž×•×“×œ×™×
    models = client.models.list()
    available_models = [m.id for m in models.data]
    print(f"âœ… Server is UP! Available models: {available_models}")

    if EXPECTED_MODEL not in available_models:
        print(f"âŒ ERROR: Config expects '{EXPECTED_MODEL}' but server has {available_models}")
        print("   -> Did you restart the ./scripts/start_local_llm.sh script after the update?")
        sys.exit(1)

    print(f"âœ… Model '{EXPECTED_MODEL}' found.")

    # 2. ×‘×“×™×§×ª ×™×¦×™×¨×ª ×˜×§×¡×˜ (×¢×‘×¨×™×ª)
    print("ðŸ’¬ Sending test prompt in Hebrew...")
    response = client.chat.completions.create(
        model=EXPECTED_MODEL,
        messages=[
            {"role": "system", "content": "××ª×” ×¢×•×–×¨ × ×—×ž×“."},
            {"role": "user", "content": "×”×× ××ª×” ×ž×—×•×‘×¨ ×•×©×•×ž×¢ ××•×ª×™? ×ª×¢× ×” ×‘×§×™×¦×•×¨."}
        ],
        temperature=0.7
    )

    answer = response.choices[0].message.content
    print(f"\nðŸ¤– Llama Response:\n{'-'*20}\n{answer}\n{'-'*20}")
    print("\nðŸš€ SUCCESS: The local brain is fully connected and ready!")

except Exception as e:
    print(f"\nâŒ CONNECTION FAILED: {e}")
    print("   -> Make sure you ran './scripts/start_local_llm.sh' in a separate terminal.")
    sys.exit(1)
