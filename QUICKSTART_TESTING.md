# TATLAM Testing Quick Start Guide

## 5-Minute Setup

### Step 1: Install Dependencies
```bash
pip install -r requirements-test.txt
```

### Step 2: Run Tests
```bash
# Option A: Use the test runner script
./run_tests.sh

# Option B: Use pytest directly
pytest tests/ -m "not slow" -v
```

### Step 3: View Results
Tests should complete in under 60 seconds (excluding slow LLM tests).

## Common Commands

### Fast Development Workflow
```bash
# Run only unit tests (< 5 seconds)
./run_tests.sh unit

# Run with coverage
./run_tests.sh coverage
```

### Before Committing
```bash
# Run fast tests (unit + integration)
./run_tests.sh fast
```

### Full Validation
```bash
# Run all tests except expensive LLM evals
./run_tests.sh all
```

## Test Output Example

```
========================================
TATLAM Test Suite
========================================

Running: Unit Tests
Command: pytest tests/unit/ -v -m unit

tests/unit/test_categories.py::TestCategories::test_cats_dictionary_exists PASSED
tests/unit/test_categories.py::TestCategories::test_cats_keys_are_hebrew PASSED
tests/unit/test_validators.py::TestValidators::test_validate_json_schema PASSED
...

âœ“ Unit Tests PASSED

Running: Integration Tests
...
```

## Troubleshooting

### "pytest: command not found"
```bash
pip install pytest
# or
pip install -r requirements-test.txt
```

### "ModuleNotFoundError: No module named 'tatlam'"
```bash
# Ensure you're in the project root
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
pytest
```

### "Database is locked"
Tests use in-memory databases. This error indicates a test didn't clean up properly.
Run tests individually to identify the culprit:
```bash
pytest tests/integration/infra/test_db_schema.py -v
```

## What Gets Tested

âœ… **Unit Tests**: Core logic (validators, categories, utilities)
âœ… **Integration Tests**: Database operations, bundling, exports
âœ… **Security Tests**: SQL injection, secrets management
âœ… **Performance Tests**: Concurrency, benchmarks

âŒ **LLM Evals** (Skipped by default - requires API keys)

## File Structure Quick Reference

```
tests/
â”œâ”€â”€ conftest.py          # Fixtures: in_memory_db, mock_brain
â”œâ”€â”€ unit/                # Fast isolated tests
â”œâ”€â”€ integration/         # Database + component tests
â”œâ”€â”€ security/            # Security validation
â”œâ”€â”€ performance/         # Benchmarks
â””â”€â”€ llm_evals/          # LLM quality (expensive)
```

## Next Steps

1. âœ… Tests are installed and ready
2. ðŸ“Š Run `./run_tests.sh coverage` to see coverage report
3. ðŸ“ Read `tests/README.md` for detailed documentation
4. ðŸš€ Enable LLM eval tests when you have API keys configured

## Quick Tips

- **Fast feedback**: Run `./run_tests.sh unit` during development
- **Before push**: Run `./run_tests.sh fast` to catch issues
- **CI/CD**: Use `./run_tests.sh ci` in pipelines
- **Deep dive**: Use `./run_tests.sh coverage` to find gaps

## Success Criteria

After running `./run_tests.sh`:
- All unit tests pass âœ…
- All integration tests pass âœ…
- All security tests pass âœ…
- Total execution time < 60 seconds âœ…

## Help

For detailed information:
- ðŸ“– See `tests/README.md`
- ðŸ“‹ See `TEST_SUITE_SUMMARY.md`
- ðŸ’» Run `./run_tests.sh help`

Happy testing! ðŸŽ‰
