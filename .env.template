# ---- Flask / Web ----
FLASK_SECRET_KEY=change-me
ADMIN_USER=
ADMIN_PASS=
SESSION_COOKIE_SECURE=false

# ---- Database ----
DB_PATH=./db/tatlam.db
TABLE_NAME=scenarios
EMB_TABLE=title_embeddings

# ---- Logging ----
LOG_LEVEL=INFO
LOG_STRUCTURED=0
LOG_FILE=

# ---- OpenAI Cloud ----
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=
GEN_MODEL=gpt-5
VALIDATOR_MODEL=gpt-5-mini
CHECKER_MODEL=gpt-5-mini
EMBED_MODEL=text-embedding-3-small

# ---- Local OpenAI-Compatible Server ----
# IMPORTANT: Port changed from 8080 to 8000 for hardware parallelism optimization
LOCAL_BASE_URL=http://127.0.0.1:8000/v1
LOCAL_API_KEY=sk-local
LOCAL_MODEL=qwen-2.5-32b-instruct
# llama.cpp server (llama-server) settings for start_local_llm.sh
# Path to your GGUF model file. Example: /Models/llama-3.1/llama-3.1-8b-instruct.Q5_K_M.gguf
LOCAL_MODEL_PATH=
# Host/port for llama-server to bind
LOCAL_HOST=0.0.0.0
LOCAL_PORT=8000
# Performance knobs (optimized for M4 Pro 48GB)
LLM_THREADS=8
LLM_CONTEXT=8192
LLM_N_PARALLEL=8
LLM_BATCH_SIZE=512
# Optional: custom binary name/path for llama-server
# LLAMA_BIN=llama-server

# ---- Generation Tunables ----
BATCH_COUNT=5
CANDIDATE_COUNT=8
KEEP_TOP_K=5
SIM_THRESHOLD=0.88
CHAT_RETRIES=3
DIVERSITY_MAX_SIM=0.92

# ---- Gold Examples ----
GOLD_SCOPE=category
GOLD_DB_LIMIT=30
GOLD_MAX_CHARS=6000
GOLD_FS_DIR=gold_md

# ---- App Flags ----
REQUIRE_APPROVED_ONLY=0

# ---- Misc ----
TMPDIR=${HOME}/tmp
