import config_trinity
import anthropic
import google.generativeai as genai
from openai import OpenAI
from typing import Generator


class TrinityBrain:
    """
    The Trinity Brain: coordinates three AI models for scenario generation and simulation.
    - Writer (Claude): Generates scenarios with streaming
    - Judge (Gemini): Audits and evaluates scenarios
    - Simulator (Local Llama): Handles chat simulations with streaming
    """

    def __init__(self):
        """Initialize the three AI clients with error handling."""
        # 1. The Writer (Claude via Anthropic)
        self.writer_client = None
        try:
            if config_trinity.ANTHROPIC_API_KEY:
                self.writer_client = anthropic.Anthropic(
                    api_key=config_trinity.ANTHROPIC_API_KEY
                )
            else:
                print("锔  Warning: ANTHROPIC_API_KEY not set. Writer (Claude) will be unavailable.")
        except Exception as e:
            print(f"锔  Warning: Failed to initialize Anthropic client: {e}")

        # 2. The Judge (Gemini via Google)
        self.judge_client = None
        try:
            if config_trinity.GOOGLE_API_KEY:
                genai.configure(api_key=config_trinity.GOOGLE_API_KEY)
                self.judge_client = genai.GenerativeModel(config_trinity.JUDGE_MODEL_NAME)
            else:
                print("锔  Warning: GOOGLE_API_KEY not set. Judge (Gemini) will be unavailable.")
        except Exception as e:
            print(f"锔  Warning: Failed to initialize Google client: {e}")

        # 3. The Simulator (Local Llama via OpenAI-compatible API)
        self.simulator_client = None
        try:
            self.simulator_client = OpenAI(
                base_url=config_trinity.LOCAL_BASE_URL,
                api_key=config_trinity.LOCAL_API_KEY
            )
        except Exception as e:
            print(f"锔  Warning: Failed to initialize local OpenAI client: {e}")

    def generate_scenario_stream(self, prompt: str) -> Generator[str, None, None]:
        """
        Generate a scenario using the Writer (Claude) with real-time streaming.

        This method streams text as it's generated, allowing users to see the
        scenario being written in real-time instead of waiting 30+ seconds.

        Args:
            prompt: The prompt describing what scenario to generate

        Yields:
            Text chunks as they are generated in real-time

        Raises:
            RuntimeError: If writer client is not initialized
        """
        if not self.writer_client:
            raise RuntimeError("Writer (Anthropic) client not initialized. Check ANTHROPIC_API_KEY.")

        # System prompt for security scenario generation in Hebrew
        system_prompt = """转  转转 转专砖  注专转 转专 爪专转 砖专.

转驻拽: 爪专 转专砖 砖专 爪转 驻专 爪转 , 注专转, 驻专 Markdown.

**驻专 :**

```markdown
# [转专转 转专砖 注专转]

专转 专转: [/转/]
 驻转 砖: [Yes/No]
砖砖 住: [Yes/No]
拽专: [砖 拽专]
专转 住: [/转// ]
专转 住专转: [/转/]

** 住驻专 拽专:**

[转专 驻专 砖 专注 - , , 驻, 转, . 转 驻住拽转 拽爪专转 专专转]

** 砖 转:**

[专砖 住驻专转  转 砖 砖 转 专砖]

**  驻转 砖:**

[住专 转 转专/住专 驻转 砖,  " 专"]

** 砖砖 住:**

[住专  专砖 砖砖 住转  注]

** 专拽注 爪注:**

[注 专拽注 专,  ""]
```

**注拽专转 砖:**
1. 转 注专转 转拽转 专专
2. 转专砖 爪专 转 爪转 住住 注  转
3. 砖 转  转 住驻爪驻 注砖
4. 拽驻 注 驻专 拽 注  砖转
5. 砖转砖 ' 转 (    )
6. 专转 专转/住/住专转 转 转 驻砖专转 驻专转
7. 转砖转 Yes/No 转 """

        try:
            with self.writer_client.messages.stream(
                model=config_trinity.WRITER_MODEL_NAME,
                max_tokens=4096,
                temperature=0.8,  # Creative but consistent
                system=system_prompt,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            ) as stream:
                for text in stream.text_stream:
                    yield text
        except Exception as e:
            raise RuntimeError(f"Failed to generate scenario stream: {e}")

    def audit_scenario(self, text: str) -> str:
        """
        Audit and evaluate a scenario using the Judge (Gemini).

        Provides professional quality control and validation of security scenarios.

        Args:
            text: The scenario text to audit (in markdown format)

        Returns:
            Detailed audit results with ratings and recommendations

        Raises:
            RuntimeError: If judge client is not initialized
        """
        if not self.judge_client:
            raise RuntimeError("Judge (Gemini) client not initialized. Check GOOGLE_API_KEY.")

        audit_prompt = f"""转 拽专 拽爪注 (auditor) 转专砖  转专 爪专转.

转驻拽: 拽 注专 转 转 转专砖  转转 砖 .

**拽专专 拽:**

1. **注 驻专** (0-10):   砖转 专砖 拽 转拽?
2. **爪转转** (0-10):  转专砖 住专 住住 注  转?
3. **专转** (0-10):  转专 专专  爪转 ?
4. **注砖转** (0-10):  砖 转 住驻爪驻 转 砖?
5. **砖转** (0-10):   注 专砖 驻 专注 拽?

**转专砖 拽专转:**

{text}

---

**专转 拽专转:**
- 专  拽专专 (0-10)
- 转 爪  (爪注)
- 爪 拽转 拽
- 爪 拽转 砖驻专
- 抓 注 转拽 住驻爪驻  专砖
- 转 注专转 转拽转 专专"""

        try:
            response = self.judge_client.generate_content(audit_prompt)
            return response.text
        except Exception as e:
            raise RuntimeError(f"Failed to audit scenario: {e}")

    def chat_simulation_stream(self, messages: list[dict]) -> Generator[str, None, None]:
        """
        Run a chat simulation using the Simulator (Local Llama) with streaming.

        Args:
            messages: List of message dicts with 'role' and 'content' keys

        Yields:
            Text chunks as they are generated

        Raises:
            RuntimeError: If simulator client is not initialized
        """
        if not self.simulator_client:
            raise RuntimeError("Simulator (Local) client not initialized. Check LOCAL_BASE_URL.")

        try:
            stream = self.simulator_client.chat.completions.create(
                model=config_trinity.LOCAL_MODEL_NAME,
                messages=messages,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            raise RuntimeError(f"Failed to run chat simulation: {e}")
