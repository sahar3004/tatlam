import streamlit as st
import asyncio
import sys
import os
import requests
from loguru import logger

# Fix Imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# Try Imports
try:
    from tatlam.core.llm_factory import create_simulator_client, create_writer_client
    from tatlam.core.brain import TrinityBrain
    from tatlam.infra.db import init_db
    from tatlam.settings import get_settings
except ImportError as e:
    st.error(f"Critical Error: Could not load system core. {e}")
    st.stop()

# Get settings
settings = get_settings()

# --- PAGE CONFIG ---
st.set_page_config(
    page_title="××¢×¨×›×ª ×ª×ª×œ×´× - ××¨×›×– ×©×œ×™×˜×”",
    page_icon="ğŸ‡®ğŸ‡±",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- RTL & STYLING ---
def apply_rtl_style():
    st.markdown("""
        <style>
        .stApp { direction: rtl; text-align: right; }
        h1, h2, h3, h4, h5, h6, p, div, span { text-align: right; font-family: 'Segoe UI', Tahoma, sans-serif; }
        .stSidebar { text-align: right; direction: rtl; }
        [data-testid="stMetricValue"] { direction: ltr; text-align: right; }
        .stTextInput input, .stTextArea textarea { direction: rtl; text-align: right; }
        /* JSON LTR fix */
        .stJson { direction: ltr; text-align: left; }
        </style>
    """, unsafe_allow_html=True)

apply_rtl_style()

# --- SYSTEM HEALTH CHECK ---
@st.cache_resource
def system_check():
    status = {"db": False, "local": False}
    try:
        init_db()
        status["db"] = True
    except Exception:
        pass

    try:
        r = requests.get("http://localhost:8080/health", timeout=0.5)
        if r.status_code == 200:
            status["local"] = True
    except Exception:
        pass
    return status

status = system_check()

# --- SIDEBAR ---
with st.sidebar:
    st.title("ğŸ¯ × ×™×•×•×˜ ××¢×¨×›×ª")

    if status["db"]:
        st.success("âœ… ×‘×¡×™×¡ × ×ª×•× ×™× (××—×•×‘×¨)")
    else:
        st.error("âŒ ×‘×¡×™×¡ × ×ª×•× ×™× (×ª×§×œ×”)")

    if status["local"]:
        st.success("âœ… ××•×— ××§×•××™ (Qwen 32B)")
    else:
        st.error("âŒ ××•×— ××§×•××™ (×× ×•×ª×§)")
        st.warning("×™×© ×œ×”×¨×™×¥ ×‘×˜×¨××™× ×œ:\n`./run_engine.sh`")

    st.markdown("---")
    st.markdown("### ×”×’×“×¨×•×ª ×™×™×¦×•×¨")
    mode = st.radio(
        "××¦×‘ ×¢×‘×•×“×”:",
        ["ğŸš€ ×˜×™×•×˜×” ××”×™×¨×” (××§×•××™)", "âœ¨ ×¢×™×‘×•×“ ××œ× (×¢× ×Ÿ + ××§×•××™)"],
        index=1
    )

# --- MAIN LOGIC (Hybrid Engine) ---
st.title("ğŸ›¡ï¸ ××—×•×œ×œ ×ª×¨×—×™×©×™× ××‘×¦×¢×™ - ×“×•×¨ 2025")
st.caption("×× ×•×¢ ×”×™×‘×¨×™×“×™: M4 Pro Local Engine + Cloud Refinery")

col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("×¤×¨××˜×¨×™×")
    category = st.selectbox("×§×˜×’×•×¨×™×™×ª ××™×¨×•×¢",
                          ["×—×¤×¥ ×—×©×•×“", "×¤×—×´×¢", "×¡×™×™×‘×¨ ×•×”×©×¤×¢×”", "×¨×¢×™×“×ª ××“××”", "×—×“×™×¨×ª ××—×‘×œ×™×", "××™×¨×•×¢ ×¨×‘-×–×™×¨×ª×™"])
    threat_level = st.select_slider("×¨××ª ××™×•×", options=["×©×’×¨×”", "×—×©×“ × ××•×š", "×’×‘×•×”", "×§×¨×™×˜×™ (×—×™×¨×•×)"])
    location = st.text_input("××™×§×•×", "×§× ×™×•×Ÿ ×¢×–×¨×™××œ×™, ×ª×œ ××‘×™×‘")

with col2:
    st.subheader("××•×“×™×¢×™×Ÿ ×•×¨×§×¢")
    base_context = st.text_area("×ª×™××•×¨ ×—×•×¤×©×™",
                                value="×”×ª×§×‘×œ ×“×™×•×•×— ×¢×œ ×“××•×ª ×—×©×•×“×” ×”××¦×œ××ª ××ª ×¢××“×•×ª ×”×××‘×˜×—×™×...",
                                height=150)


class HybridBrain:
    """Hybrid Brain: Local Drafter + Cloud Refiner."""

    def __init__(self, local_client=None, refiner_client=None):
        self.local_client = local_client
        self.refiner_client = refiner_client
        self._settings = get_settings()

    async def generate_draft(self, category: str, threat_level: str, context: str) -> dict:
        """Generate initial draft using local LLM."""
        if not self.local_client:
            raise RuntimeError("Local client not initialized")

        prompt = f"""××ª×” ××•××—×” ××‘×˜×—×” ×™×©×¨××œ×™. ×¦×•×¨ ×ª×¨×—×™×© ××™××•×Ÿ ××‘×¦×¢×™.

×§×˜×’×•×¨×™×”: {category}
×¨××ª ××™×•×: {threat_level}
×”×§×©×¨: {context}

×”×—×–×¨ JSON ×¢× ×”×©×“×•×ª ×”×‘××™×:
- title: ×›×•×ª×¨×ª ×”×ª×¨×—×™×©
- background: ×¨×§×¢ ××‘×¦×¢×™
- steps: ×¨×©×™××ª ×©×œ×‘×™ ×”××™×¨×•×¢ (××¢×¨×š)
- required_response: ×ª×’×•×‘×” × ×“×¨×©×ª (××¢×¨×š)
- threat_level: ×¨××ª ×”××™×•×

×”×—×–×¨ JSON ×‘×œ×‘×“, ×œ×œ× ×˜×§×¡×˜ × ×•×¡×£."""

        response = self.local_client.chat.completions.create(
            model=self._settings.LOCAL_MODEL_NAME,
            messages=[
                {"role": "system", "content": "××ª×” ××•××—×” ××‘×˜×—×” ××‘×¦×¢×™×ª. ×”×—×–×¨ JSON ×‘×œ×‘×“."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048,
            response_format={"type": "json_object"}
        )

        import json
        content = response.choices[0].message.content
        return json.loads(content)

    async def refine_scenario(self, draft: dict) -> dict:
        """Refine draft using cloud LLM (Claude)."""
        if not self.refiner_client:
            # No cloud - return draft as-is
            return draft

        import json

        prompt = f"""×‘×¦×¢ ×œ×™×˜×•×© ×“×•×§×˜×¨×™× ×¨×™ ×œ×ª×¨×—×™×© ×”××™××•×Ÿ ×”×‘×.
×©×¤×¨ ××ª ×”× ×™×¡×•×—, ×”×•×¡×£ ×¤×¨×˜×™× ××‘×¦×¢×™×™×, ×•×•×“× ×”×ª×××” ×œ×¡×˜× ×“×¨×˜×™× ××§×¦×•×¢×™×™×.

×ª×¨×—×™×© ××§×•×¨×™:
{json.dumps(draft, ensure_ascii=False, indent=2)}

×”×—×–×¨ JSON ××©×•×¤×¨ ×¢× ××•×ª× ×©×“×•×ª."""

        response = self.refiner_client.messages.create(
            model=self._settings.WRITER_MODEL_NAME,
            max_tokens=4096,
            messages=[{"role": "user", "content": prompt}]
        )

        # Extract JSON from response
        content = response.content[0].text

        # Try to parse JSON from response
        try:
            # Find JSON in response
            start = content.find('{')
            end = content.rfind('}') + 1
            if start >= 0 and end > start:
                return json.loads(content[start:end])
        except json.JSONDecodeError:
            pass

        # If parsing fails, return original draft with refinement note
        draft['refinement_note'] = content
        return draft


async def execute_mission():
    placeholder = st.empty()
    logs = st.status("ğŸ”„ ××¢×¨×›×ª ×‘×¤×¢×•×œ×”...", expanded=True)

    try:
        # 1. Local Drafter
        logs.write("ğŸ§  ××¤×¢×™×œ ×× ×•×¢ ××§×•××™ (Drafter)...")
        local_client = create_simulator_client()

        # 2. Cloud Refiner (optional)
        cloud_client = None
        if "×¢×™×‘×•×“ ××œ×" in mode:
            try:
                logs.write("â˜ï¸ ×™×•×¦×¨ ×§×©×¨ ×¢× ×”×¢× ×Ÿ (Refiner)...")
                cloud_client = create_writer_client()
            except Exception as e:
                logs.warning(f"××™×Ÿ ×—×™×‘×•×¨ ×œ×¢× ×Ÿ: {e}. ×××©×™×š ××§×•××™.")

        brain = HybridBrain(local_client=local_client, refiner_client=cloud_client)

        # 3. Execution
        logs.write("ğŸ“ ××™×™×¦×¨ ×©×œ×“ ×ª×¨×—×™×©...")
        draft = await brain.generate_draft(
            category=category,
            threat_level=threat_level,
            context=f"××™×§×•×: {location}. {base_context}"
        )

        if cloud_client:
            logs.write("âœ¨ ××‘×¦×¢ ×œ×™×˜×•×© ×“×•×§×˜×¨×™× ×¨×™...")
            final = await brain.refine_scenario(draft)
            logs.update(label="âœ… ×¡×™×•× ××•×¦×œ×—!", state="complete", expanded=False)
            return final
        else:
            logs.update(label="âœ… ×˜×™×•×˜×” ××•×›× ×”", state="complete", expanded=False)
            return draft

    except Exception as e:
        logs.update(label="âŒ ×©×’×™××”", state="error")
        st.error(f"Error: {str(e)}")
        logger.exception(f"Mission execution failed: {e}")
        return None


if st.button("×¦×•×¨ ×ª×¨×—×™×© ×›×¢×ª âš¡", type="primary", use_container_width=True):
    if not status["local"]:
        st.error("×”××•×“×œ ×”××§×•××™ ×›×‘×•×™. ×× × ×”×¤×¢×œ ××ª ×”×©×¨×ª ×¢× ./run_engine.sh")
    else:
        result = asyncio.run(execute_mission())
        if result:
            st.divider()
            t1, t2 = st.tabs(["×ª×¦×•×’×” ××‘×¦×¢×™×ª", "JSON"])
            with t1:
                st.subheader(result.get('title', '×œ×œ× ×›×•×ª×¨×ª'))
                st.info(result.get('background', ''))

                steps = result.get('steps', [])
                if steps:
                    st.markdown("**×©×œ×‘×™ ×”××™×¨×•×¢:**")
                    for i, step in enumerate(steps, 1):
                        if isinstance(step, dict):
                            st.text(f"{i}. {step.get('time', '')} - {step.get('description', step)}")
                        else:
                            st.text(f"{i}. {step}")

                response = result.get('required_response', [])
                if response:
                    st.markdown("**×ª×’×•×‘×” × ×“×¨×©×ª:**")
                    for item in response:
                        st.text(f"â€¢ {item}")

            with t2:
                st.json(result)
